#!/bin/bash

# Example script to run DeepRecInfra.
# This allows you to run the neural recommendation models found in models/
# along with the recommendation load generator to measure inference
# tail-latency It does not include the query scheduler which would
# optimize for latency-bounded throughput (inference QPS)

###############################################################################

########## Epoch args ##############
# The total number of inference queries run is product of the number of epochs and number of batches
# Inference querues = nepochs * num_batches
# The number of batches is the unique number of data inputs generated by the data generators
# The number of epochs of epochs determines how many iterations to loop over
# Configuring these parameters is important to getting accurate caching
# behavior based on the use case being modeled
nepochs=128
num_batches=32

epoch_args="--nepochs $nepochs --num_batches $num_batches"

# Configuration for hill-climbing based scheduler
model_config="models/configs/wide_and_deep.json"
target_latency=25 # target p95 tail latency in terms of ms

########## Inference engine args ##############

# The number of inference engines determines the unique number of Caffe2
# CPU processes to parallelize queries over
inference_engines=32
caffe2_net_type="async_dag"

engine_args="--inference_engines $inference_engines --caffe2_net_type $caffe2_net_type"

########## Query size args ##############
# Configuration for query sizes.
batch_size_distribution="lognormal" # number of candidate items in query follows normal distribution
max_mini_batch_size=1024 # maximum number of candidate items queries
avg_mini_batch_size=5.1 # mean number of canidate items in query
var_mini_batch_size=0.2 # variation of number of canidate items in query
sub_task_batch_size=64 # per-core query size (number of items processed per-core)

batch_args="--batch_size_distribution $batch_size_distribution --max_mini_batch_size $max_mini_batch_size --avg_mini_batch_size $avg_mini_batch_size --var_mini_batch_size $var_mini_batch_size --sub_task_batch_size $sub_task_batch_size"

########## Scheduling args ##############

# Configuring input arrival rates for hill-climbing scheduler to iterate over
min_arr_range=1   # minimum input query arrival rate (Poisson distribution) in terms of ms
max_arr_range=20  # maximum input query arrival rate (Poisson distribution) in terms of ms
arr_steps=50      # Number of steps between min. and max. arrival rate to try equally spaced based on logspace

# Optimal batching configurations to try (task vs. request or data-level parallelism)
# These are all the per-core query or batch-sizes for the hill-climbing scheduler to try.
batch_configs="512-384-256-192-128-96-64-48-32"

# Parameters for hill-climbing scheduler to find steady state
req_granularity=64 # number of queries after which scheduler updates arrival rates
sched_timeout=128 # number of updates after which scheduler scheduler times out finding optimal point

scheduler_args="--target_latency $target_latency --min_arr_range $min_arr_range --max_arr_range $max_arr_range --arr_steps $arr_steps --batch_configs $batch_configs --req_granularity $req_granularity --sched_timeout $sched_timeout"

########## Accelerator arguments ##############
accel_configs="96-128-192-256-384-512"
accel_args="--accel_configs $accel_configs --model_accel"
###############################################################################

mkdir -p log/
CWD=$(pwd)
echo $CWD
cd ../../

for seed in `seq 0 5`; do

# Only CPU
python DeepRecSys.py $epoch_args $engine_args $batch_args $scheduler_args --queue --config_file $model_config --tune_batch_qps

# CPU and GPU
python DeepRecSys.py $epoch_args $engine_args $batch_args $scheduler_args $accel_args --queue --config_file $model_config --tune_batch_qps --tune_accel_qps

done
